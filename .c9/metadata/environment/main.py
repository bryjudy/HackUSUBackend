{"filter":false,"title":"main.py","tooltip":"/main.py","undoManager":{"mark":100,"position":100,"stack":[[{"start":{"row":175,"column":0},"end":{"row":204,"column":0},"action":"remove","lines":["def main():","    # Load data","    df = pd.read_csv('twilio.csv')","    ","    # Initialize classes with the DataFrame","    preprocessor = DataPreprocessor(df)","    sentiment_analyzer = SentimentAnalyzer(df)","    aspect_extractor = AspectExtractor(df)","    visualizer = DataVisualizer(df)","    ","    # Run processing steps","    preprocessor.clean_data()","    preprocessor.convert_response_delay()","    sentiment_analyzer.categorize_sentiment()","    aspect_extractor.extract_aspects()","    ","    # Visualization","    visualizer.plot_responses_over_time()","    visualizer.plot_sentiment_trend_over_time()","    ","    # Analysis and output","    tag_counts = visualizer.generate_tag_counts()","    highlight_reviews = visualizer.identify_highlight_reviews()","    ","    # Optionally, export highlight reviews to JSON","    visualizer.export_to_json(highlight_reviews, 'highlight_reviews.json')","","if __name__ == \"__main__\":","    main()",""],"id":460},{"start":{"row":175,"column":0},"end":{"row":209,"column":10},"action":"insert","lines":["def main():","    # Step 1: Load the dataset","    dataframe = pd.read_csv('survey_data.csv')","    ","    # Initialize the classes with the DataFrame","    preprocessor = DataPreprocessor(dataframe)","    sentiment_analyzer = SentimentAnalyzer(dataframe)","    aspect_extractor = AspectExtractor(dataframe)","    visualizer = DataVisualizer(dataframe)","    ","    # Step 2: Preprocess the data","    preprocessor.clean_data()","    preprocessor.convert_response_delay()","    ","    # Step 3: Analyze sentiments","    sentiment_analyzer.categorize_sentiment()","    ","    # Step 4: Extract aspects","    aspect_extractor.extract_aspects()","    ","    # Step 5: Visualize data","    visualizer.visualize_sentiments_over_delay()","    ","    # Optional Steps:","    # Generate tag counts","    tag_counts = visualizer.generate_tag_counts()","    ","    # Identify highlight reviews","    highlight_reviews = visualizer.identify_highlight_reviews()","    ","    # Export results to JSON (example for highlight reviews)","    visualizer.export_to_json(highlight_reviews, 'highlight_reviews.json')","","if __name__ == '__main__':","    main()"]}],[{"start":{"row":177,"column":29},"end":{"row":177,"column":40},"action":"remove","lines":["survey_data"],"id":461},{"start":{"row":177,"column":29},"end":{"row":177,"column":30},"action":"insert","lines":["t"]},{"start":{"row":177,"column":30},"end":{"row":177,"column":31},"action":"insert","lines":["w"]},{"start":{"row":177,"column":31},"end":{"row":177,"column":32},"action":"insert","lines":["i"]},{"start":{"row":177,"column":32},"end":{"row":177,"column":33},"action":"insert","lines":["l"]},{"start":{"row":177,"column":33},"end":{"row":177,"column":34},"action":"insert","lines":["i"]},{"start":{"row":177,"column":34},"end":{"row":177,"column":35},"action":"insert","lines":["i"]}],[{"start":{"row":177,"column":34},"end":{"row":177,"column":35},"action":"remove","lines":["i"],"id":462}],[{"start":{"row":177,"column":34},"end":{"row":177,"column":35},"action":"insert","lines":["o"],"id":463}],[{"start":{"row":157,"column":0},"end":{"row":165,"column":23},"action":"remove","lines":["        def find_highlight_review(tags_dict, sentiment_filter):","            if tags_dict:","                most_popular_tag = next(iter(tags_dict))","                # Filter dataframe for the sentiment and check if the most popular tag is in the tags list","                filtered_df = self.dataframe[self.dataframe['tag_list'].apply(lambda x: most_popular_tag in x) & sentiment_filter]","                if not filtered_df.empty:","                    # Sort by datetime and take the most recent","                    return filtered_df.sort_values(by='datetime', ascending=False).iloc[0]['question_2_response']","            return None"],"id":464},{"start":{"row":157,"column":0},"end":{"row":165,"column":15},"action":"insert","lines":["def find_highlight_review(self, tags_dict, sentiment_filter):","    if tags_dict:","        most_popular_tag = next(iter(tags_dict))","        # Filter dataframe for the sentiment and check if the most popular tag is in the tags list","        filtered_df = self.dataframe[self.dataframe['tag_list'].apply(lambda x: most_popular_tag in x) & sentiment_filter]","        if not filtered_df.empty:","            # Sort by response_delay (ascending) to select the review with the shortest delay","            return filtered_df.sort_values(by='response_delay', ascending=True).iloc[0]['question_2_response']","    return None"]}],[{"start":{"row":157,"column":0},"end":{"row":157,"column":4},"action":"insert","lines":["    "],"id":465},{"start":{"row":158,"column":0},"end":{"row":158,"column":4},"action":"insert","lines":["    "]},{"start":{"row":159,"column":0},"end":{"row":159,"column":4},"action":"insert","lines":["    "]},{"start":{"row":160,"column":0},"end":{"row":160,"column":4},"action":"insert","lines":["    "]},{"start":{"row":161,"column":0},"end":{"row":161,"column":4},"action":"insert","lines":["    "]},{"start":{"row":162,"column":0},"end":{"row":162,"column":4},"action":"insert","lines":["    "]},{"start":{"row":163,"column":0},"end":{"row":163,"column":4},"action":"insert","lines":["    "]},{"start":{"row":164,"column":0},"end":{"row":164,"column":4},"action":"insert","lines":["    "]},{"start":{"row":165,"column":0},"end":{"row":165,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":146,"column":0},"end":{"row":172,"column":32},"action":"remove","lines":["    def identify_highlight_reviews(self):","        # Assuming generate_tag_counts has been called and we have dictionaries for tag counts","        positive_tags, neutral_tags, negative_tags = self.generate_tag_counts()","        ","        highlight_reviews = {","            'positive': None,","            'neutral': None,","            'negative': None","        }","    ","        # Function to find the highlight review for a given sentiment","    def find_highlight_review(self, tags_dict, sentiment_filter):","        if tags_dict:","            most_popular_tag = next(iter(tags_dict))","            # Filter dataframe for the sentiment and check if the most popular tag is in the tags list","            filtered_df = self.dataframe[self.dataframe['tag_list'].apply(lambda x: most_popular_tag in x) & sentiment_filter]","            if not filtered_df.empty:","                # Sort by response_delay (ascending) to select the review with the shortest delay","                return filtered_df.sort_values(by='response_delay', ascending=True).iloc[0]['question_2_response']","        return None","    ","        # Identify highlight reviews for each sentiment","        highlight_reviews['positive'] = find_highlight_review(positive_tags, self.dataframe['is_positive'] == 1)","        highlight_reviews['neutral'] = find_highlight_review(neutral_tags, self.dataframe['is_neutral'] == 1)","        highlight_reviews['negative'] = find_highlight_review(negative_tags, self.dataframe['is_negative'] == 1)","    ","        return highlight_reviews"],"id":466},{"start":{"row":146,"column":0},"end":{"row":166,"column":32},"action":"insert","lines":["    def find_highlight_review(self, tags_dict, sentiment_filter):","        if tags_dict:","            most_popular_tag = next(iter(tags_dict))","            # Filter dataframe for the sentiment and check if the most popular tag is in the tags list","            filtered_df = self.dataframe[self.dataframe['tag_list'].apply(lambda x: most_popular_tag in x) & sentiment_filter]","            if not filtered_df.empty:","                # Sort by response_delay (ascending) to select the review with the shortest delay","                return filtered_df.sort_values(by='response_delay', ascending=True).iloc[0]['question_2_response']","        return None","","    def identify_highlight_reviews(self):","        # Assume generate_tag_counts has been called and we have dictionaries for tag counts","        positive_tags, neutral_tags, negative_tags = self.generate_tag_counts()","","        highlight_reviews = {","            'positive': self.find_highlight_review(positive_tags, self.dataframe['is_positive'] == 1),","            'neutral': self.find_highlight_review(neutral_tags, self.dataframe['is_neutral'] == 1),","            'negative': self.find_highlight_review(negative_tags, self.dataframe['is_negative'] == 1)","        }","","        return highlight_reviews"]}],[{"start":{"row":169,"column":0},"end":{"row":205,"column":0},"action":"remove","lines":["def main():","    # Step 1: Load the dataset","    dataframe = pd.read_csv('twilio.csv')","    ","    # Initialize the classes with the DataFrame","    preprocessor = DataPreprocessor(dataframe)","    sentiment_analyzer = SentimentAnalyzer(dataframe)","    aspect_extractor = AspectExtractor(dataframe)","    visualizer = DataVisualizer(dataframe)","    ","    # Step 2: Preprocess the data","    preprocessor.clean_data()","    preprocessor.convert_response_delay()","    ","    # Step 3: Analyze sentiments","    sentiment_analyzer.categorize_sentiment()","    ","    # Step 4: Extract aspects","    aspect_extractor.extract_aspects()","    ","    # Step 5: Visualize data","    visualizer.visualize_sentiments_over_delay()","    ","    # Optional Steps:","    # Generate tag counts","    tag_counts = visualizer.generate_tag_counts()","    ","    # Identify highlight reviews","    highlight_reviews = visualizer.identify_highlight_reviews()","    ","    # Export results to JSON (example for highlight reviews)","    visualizer.export_to_json(highlight_reviews, 'highlight_reviews.json')","","if __name__ == '__main__':","    main()","",""],"id":467},{"start":{"row":169,"column":0},"end":{"row":211,"column":10},"action":"insert","lines":["def main():","    try:","        # Step 1: Load the dataset","        dataframe = pd.read_csv('twilio.csv')","    except FileNotFoundError:","        print(\"Error: The dataset file 'twilio.csv' was not found.\")","        return","    except Exception as e:","        print(f\"An unexpected error occurred while loading the dataset: {e}\")","        return","","    try:","        # Initialize the classes with the DataFrame","        preprocessor = DataPreprocessor(dataframe)","        sentiment_analyzer = SentimentAnalyzer(dataframe)","        aspect_extractor = AspectExtractor(dataframe)","        visualizer = DataVisualizer(dataframe)","","        # Step 2: Preprocess the data","        preprocessor.clean_data()","        preprocessor.convert_response_delay()","","        # Step 3: Analyze sentiments","        sentiment_analyzer.categorize_sentiment()","","        # Step 4: Extract aspects","        aspect_extractor.extract_aspects()","","        # Step 5: Visualize data","        visualizer.visualize_sentiments_over_delay()","","        # Optional Steps:","        # Generate tag counts and identify highlight reviews","        tag_counts = visualizer.generate_tag_counts()","        highlight_reviews = visualizer.identify_highlight_reviews()","","        # Export results to JSON (example for highlight reviews)","        visualizer.export_to_json(highlight_reviews, 'highlight_reviews.json')","    except Exception as e:","        print(f\"An unexpected error occurred: {e}\")","","if __name__ == '__main__':","    main()"]}],[{"start":{"row":172,"column":33},"end":{"row":172,"column":34},"action":"insert","lines":["h"],"id":468},{"start":{"row":172,"column":34},"end":{"row":172,"column":35},"action":"insert","lines":["o"]},{"start":{"row":172,"column":35},"end":{"row":172,"column":36},"action":"insert","lines":["m"]},{"start":{"row":172,"column":36},"end":{"row":172,"column":37},"action":"insert","lines":["e"]},{"start":{"row":172,"column":37},"end":{"row":172,"column":38},"action":"insert","lines":["."]}],[{"start":{"row":172,"column":37},"end":{"row":172,"column":38},"action":"remove","lines":["."],"id":469}],[{"start":{"row":172,"column":37},"end":{"row":172,"column":38},"action":"insert","lines":["/"],"id":470},{"start":{"row":172,"column":38},"end":{"row":172,"column":39},"action":"insert","lines":["u"]},{"start":{"row":172,"column":39},"end":{"row":172,"column":40},"action":"insert","lines":["b"]},{"start":{"row":172,"column":40},"end":{"row":172,"column":41},"action":"insert","lines":["u"]},{"start":{"row":172,"column":41},"end":{"row":172,"column":42},"action":"insert","lines":["n"]},{"start":{"row":172,"column":42},"end":{"row":172,"column":43},"action":"insert","lines":["t"]},{"start":{"row":172,"column":43},"end":{"row":172,"column":44},"action":"insert","lines":["u"]}],[{"start":{"row":172,"column":44},"end":{"row":172,"column":45},"action":"insert","lines":["/"],"id":471},{"start":{"row":172,"column":45},"end":{"row":172,"column":46},"action":"insert","lines":["e"]},{"start":{"row":172,"column":46},"end":{"row":172,"column":47},"action":"insert","lines":["n"]},{"start":{"row":172,"column":47},"end":{"row":172,"column":48},"action":"insert","lines":["v"]},{"start":{"row":172,"column":48},"end":{"row":172,"column":49},"action":"insert","lines":["i"]},{"start":{"row":172,"column":49},"end":{"row":172,"column":50},"action":"insert","lines":["r"]},{"start":{"row":172,"column":50},"end":{"row":172,"column":51},"action":"insert","lines":["o"]},{"start":{"row":172,"column":51},"end":{"row":172,"column":52},"action":"insert","lines":["n"]}],[{"start":{"row":172,"column":52},"end":{"row":172,"column":53},"action":"insert","lines":["m"],"id":472},{"start":{"row":172,"column":53},"end":{"row":172,"column":54},"action":"insert","lines":["e"]},{"start":{"row":172,"column":54},"end":{"row":172,"column":55},"action":"insert","lines":["n"]},{"start":{"row":172,"column":55},"end":{"row":172,"column":56},"action":"insert","lines":["t"]},{"start":{"row":172,"column":56},"end":{"row":172,"column":57},"action":"insert","lines":["/"]}],[{"start":{"row":172,"column":56},"end":{"row":172,"column":57},"action":"remove","lines":["/"],"id":473},{"start":{"row":172,"column":55},"end":{"row":172,"column":56},"action":"remove","lines":["t"]},{"start":{"row":172,"column":54},"end":{"row":172,"column":55},"action":"remove","lines":["n"]},{"start":{"row":172,"column":53},"end":{"row":172,"column":54},"action":"remove","lines":["e"]},{"start":{"row":172,"column":52},"end":{"row":172,"column":53},"action":"remove","lines":["m"]},{"start":{"row":172,"column":51},"end":{"row":172,"column":52},"action":"remove","lines":["n"]},{"start":{"row":172,"column":50},"end":{"row":172,"column":51},"action":"remove","lines":["o"]},{"start":{"row":172,"column":49},"end":{"row":172,"column":50},"action":"remove","lines":["r"]},{"start":{"row":172,"column":48},"end":{"row":172,"column":49},"action":"remove","lines":["i"]},{"start":{"row":172,"column":47},"end":{"row":172,"column":48},"action":"remove","lines":["v"]},{"start":{"row":172,"column":46},"end":{"row":172,"column":47},"action":"remove","lines":["n"]},{"start":{"row":172,"column":45},"end":{"row":172,"column":46},"action":"remove","lines":["e"]},{"start":{"row":172,"column":44},"end":{"row":172,"column":45},"action":"remove","lines":["/"]}],[{"start":{"row":172,"column":43},"end":{"row":172,"column":44},"action":"remove","lines":["u"],"id":474},{"start":{"row":172,"column":42},"end":{"row":172,"column":43},"action":"remove","lines":["t"]},{"start":{"row":172,"column":41},"end":{"row":172,"column":42},"action":"remove","lines":["n"]},{"start":{"row":172,"column":40},"end":{"row":172,"column":41},"action":"remove","lines":["u"]},{"start":{"row":172,"column":39},"end":{"row":172,"column":40},"action":"remove","lines":["b"]},{"start":{"row":172,"column":38},"end":{"row":172,"column":39},"action":"remove","lines":["u"]}],[{"start":{"row":172,"column":37},"end":{"row":172,"column":38},"action":"remove","lines":["/"],"id":475},{"start":{"row":172,"column":36},"end":{"row":172,"column":37},"action":"remove","lines":["e"]},{"start":{"row":172,"column":35},"end":{"row":172,"column":36},"action":"remove","lines":["m"]},{"start":{"row":172,"column":34},"end":{"row":172,"column":35},"action":"remove","lines":["o"]},{"start":{"row":172,"column":33},"end":{"row":172,"column":34},"action":"remove","lines":["h"]}],[{"start":{"row":172,"column":33},"end":{"row":172,"column":34},"action":"insert","lines":["/"],"id":476}],[{"start":{"row":172,"column":33},"end":{"row":172,"column":34},"action":"remove","lines":["/"],"id":477}],[{"start":{"row":111,"column":18},"end":{"row":112,"column":0},"action":"insert","lines":["",""],"id":478},{"start":{"row":112,"column":0},"end":{"row":112,"column":8},"action":"insert","lines":["        "]},{"start":{"row":112,"column":8},"end":{"row":113,"column":0},"action":"insert","lines":["",""]},{"start":{"row":113,"column":0},"end":{"row":113,"column":8},"action":"insert","lines":["        "]}],[{"start":{"row":113,"column":8},"end":{"row":114,"column":51},"action":"insert","lines":["    plt.savefig('sentiment_distribution_over_response_delay.png')  # Save the plot to a file","    plt.close()  # Close the plot to free up memory"],"id":479}],[{"start":{"row":113,"column":1},"end":{"row":114,"column":51},"action":"remove","lines":["           plt.savefig('sentiment_distribution_over_response_delay.png')  # Save the plot to a file","    plt.close()  # Close the plot to free up memory"],"id":480}],[{"start":{"row":113,"column":0},"end":{"row":113,"column":1},"action":"remove","lines":[" "],"id":481},{"start":{"row":112,"column":8},"end":{"row":113,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":112,"column":8},"end":{"row":113,"column":51},"action":"insert","lines":["    plt.savefig('sentiment_distribution_over_response_delay.png')  # Save the plot to a file","    plt.close()  # Close the plot to free up memory"],"id":482}],[{"start":{"row":112,"column":8},"end":{"row":112,"column":12},"action":"remove","lines":["    "],"id":483}],[{"start":{"row":113,"column":4},"end":{"row":113,"column":8},"action":"insert","lines":["    "],"id":484}],[{"start":{"row":2,"column":21},"end":{"row":3,"column":0},"action":"insert","lines":["",""],"id":485},{"start":{"row":3,"column":0},"end":{"row":3,"column":1},"action":"insert","lines":["i"]},{"start":{"row":3,"column":1},"end":{"row":3,"column":2},"action":"insert","lines":["n"]},{"start":{"row":3,"column":2},"end":{"row":3,"column":3},"action":"insert","lines":["s"]},{"start":{"row":3,"column":3},"end":{"row":3,"column":4},"action":"insert","lines":["t"]},{"start":{"row":3,"column":4},"end":{"row":3,"column":5},"action":"insert","lines":["a"]},{"start":{"row":3,"column":5},"end":{"row":3,"column":6},"action":"insert","lines":["l"]},{"start":{"row":3,"column":6},"end":{"row":3,"column":7},"action":"insert","lines":["l"]}],[{"start":{"row":3,"column":7},"end":{"row":3,"column":8},"action":"insert","lines":[" "],"id":486},{"start":{"row":3,"column":8},"end":{"row":3,"column":9},"action":"insert","lines":["n"]},{"start":{"row":3,"column":9},"end":{"row":3,"column":10},"action":"insert","lines":["u"]}],[{"start":{"row":3,"column":9},"end":{"row":3,"column":10},"action":"remove","lines":["u"],"id":487},{"start":{"row":3,"column":8},"end":{"row":3,"column":9},"action":"remove","lines":["n"]},{"start":{"row":3,"column":7},"end":{"row":3,"column":8},"action":"remove","lines":[" "]},{"start":{"row":3,"column":6},"end":{"row":3,"column":7},"action":"remove","lines":["l"]},{"start":{"row":3,"column":5},"end":{"row":3,"column":6},"action":"remove","lines":["l"]},{"start":{"row":3,"column":4},"end":{"row":3,"column":5},"action":"remove","lines":["a"]},{"start":{"row":3,"column":3},"end":{"row":3,"column":4},"action":"remove","lines":["t"]},{"start":{"row":3,"column":2},"end":{"row":3,"column":3},"action":"remove","lines":["s"]},{"start":{"row":3,"column":1},"end":{"row":3,"column":2},"action":"remove","lines":["n"]}],[{"start":{"row":3,"column":1},"end":{"row":3,"column":2},"action":"insert","lines":["m"],"id":488},{"start":{"row":3,"column":2},"end":{"row":3,"column":3},"action":"insert","lines":["p"]},{"start":{"row":3,"column":3},"end":{"row":3,"column":4},"action":"insert","lines":["o"]},{"start":{"row":3,"column":4},"end":{"row":3,"column":5},"action":"insert","lines":["r"]},{"start":{"row":3,"column":5},"end":{"row":3,"column":6},"action":"insert","lines":["t"]}],[{"start":{"row":3,"column":6},"end":{"row":3,"column":7},"action":"insert","lines":[" "],"id":489},{"start":{"row":3,"column":7},"end":{"row":3,"column":8},"action":"insert","lines":["n"]},{"start":{"row":3,"column":8},"end":{"row":3,"column":9},"action":"insert","lines":["u"]},{"start":{"row":3,"column":9},"end":{"row":3,"column":10},"action":"insert","lines":["m"]},{"start":{"row":3,"column":10},"end":{"row":3,"column":11},"action":"insert","lines":["p"]},{"start":{"row":3,"column":11},"end":{"row":3,"column":12},"action":"insert","lines":["y"]}],[{"start":{"row":3,"column":12},"end":{"row":3,"column":13},"action":"insert","lines":[" "],"id":490},{"start":{"row":3,"column":13},"end":{"row":3,"column":14},"action":"insert","lines":["a"]},{"start":{"row":3,"column":14},"end":{"row":3,"column":15},"action":"insert","lines":["s"]}],[{"start":{"row":3,"column":15},"end":{"row":3,"column":16},"action":"insert","lines":[" "],"id":491},{"start":{"row":3,"column":16},"end":{"row":3,"column":17},"action":"insert","lines":["n"]},{"start":{"row":3,"column":17},"end":{"row":3,"column":18},"action":"insert","lines":["p"]}],[{"start":{"row":78,"column":0},"end":{"row":114,"column":55},"action":"remove","lines":["class DataVisualizer:","    def __init__(self, dataframe):","        self.dataframe = dataframe","    ","    def visualize_sentiments_over_delay(self):","        # Calculate sentiment distribution per response delay","        sentiment_distribution = self.dataframe.groupby('response_delay_hours').agg({","            'is_positive': 'sum',","            'is_neutral': 'sum',","            'is_negative': 'sum'","        })","","        # Dynamically calculate the total number of responses","        total_responses = self.dataframe.shape[0]  # Gets the total number of rows (responses) in the DataFrame","","        # Calculate the total counts for each response delay bucket and their percentage","        sentiment_distribution['total'] = sentiment_distribution.sum(axis=1)","        sentiment_distribution['percentage'] = (sentiment_distribution['total'] / total_responses) * 100","","        # Plotting","        fig, ax1 = plt.subplots(figsize=(10, 6))","","        # Stacked bar chart for sentiment distribution","        sentiment_distribution[['is_positive', 'is_neutral', 'is_negative']].plot(kind='bar', stacked=True, ax=ax1)","        ax1.set_ylabel('Number of Responses')","        ax1.set_xlabel('Response Delay (Hours)')","        ax1.set_title('Sentiment Distribution Over Response Delay')","","        # Line chart for percentage of total responses","        ax2 = ax1.twinx()","        ax2.plot(sentiment_distribution.index, sentiment_distribution['percentage'], color='blue', marker='o', label='Percentage of Responses')","        ax2.set_ylabel('Percentage of Total Responses (%)')","        ax2.legend(loc='upper right')","","        plt.show()","        plt.savefig('sentiment_distribution_over_response_delay.png')  # Save the plot to a file","        plt.close()  # Close the plot to free up memory"],"id":492},{"start":{"row":78,"column":0},"end":{"row":117,"column":55},"action":"insert","lines":["class DataVisualizer:","    def __init__(self, dataframe):","        self.dataframe = dataframe","    ","    def visualize_sentiments_over_delay(self):","        # Ensure response_delay_hours is numeric","        self.dataframe['response_delay_hours'] = pd.to_numeric(self.dataframe['response_delay_hours'], errors='coerce')","        ","        # Create bins for every hour of response delay","        max_hours = self.dataframe['response_delay_hours'].max()","        bins = np.arange(0, max_hours + 1, 1)  # +1 to ensure the last hour is included","        labels = [f\"{int(x)}-{int(x+1)}\" for x in bins[:-1]]  # Label each bin by its range","        self.dataframe['hourly_bins'] = pd.cut(self.dataframe['response_delay_hours'], bins=bins, labels=labels, right=False)","","        # Calculate sentiment distribution per hourly bin","        sentiment_distribution = self.dataframe.groupby('hourly_bins').agg({","            'is_positive': 'sum',","            'is_neutral': 'sum',","            'is_negative': 'sum'","        }).fillna(0)","","        # Plotting","        fig, ax1 = plt.subplots(figsize=(14, 8))","        ","        # Colors for the stacked bar chart","        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Blue for positive, Orange for neutral, Green for negative","        ","        # Stacked bar chart for sentiment distribution per hour","        sentiment_distribution.plot(kind='bar', stacked=True, color=colors, ax=ax1)","        ax1.set_ylabel('Number of Responses')","        ax1.set_xlabel('Response Delay (Hours)')","        ax1.set_title('Sentiment Distribution Over Response Delay')","        ax1.legend([\"Positive\", \"Neutral\", \"Negative\"], title=\"Sentiment\")","","        plt.xticks(rotation=45, ha=\"right\")","        plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels","        ","        plt.savefig('sentiment_distribution_over_response_delay.png')  # Save the plot to a file","        plt.show()","        plt.close()  # Close the plot to free up memory"]}],[{"start":{"row":16,"column":0},"end":{"row":33,"column":88},"action":"remove","lines":["class DataPreprocessor:","    def __init__(self, dataframe):","        self.dataframe = dataframe","    ","    def clean_data(self):","        # Remove special characters from text fields","        text_fields = ['question_1_response', 'question_2_response', 'additional_information']","        for field in text_fields:","            self.dataframe[field] = self.dataframe[field].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', str(x)))","        ","        # Fill missing values in text fields","        self.dataframe[text_fields] = self.dataframe[text_fields].fillna('missing')","        ","        # Convert text data to lowercase","        self.dataframe[text_fields] = self.dataframe[text_fields].apply(lambda x: x.str.lower())","","    def convert_response_delay(self):","        self.dataframe['response_delay_hours'] = self.dataframe['response_delay'] / 3600"],"id":493},{"start":{"row":16,"column":0},"end":{"row":45,"column":55},"action":"insert","lines":["class DataPreprocessor:","    def __init__(self, dataframe):","        self.dataframe = dataframe","","    def clean_data(self):","        text_fields = ['question_1_response', 'question_2_response', 'additional_information']","","        # Ensure text fields exist before processing","        missing_fields = set(text_fields) - set(self.dataframe.columns)","        if missing_fields:","            raise ValueError(f\"DataFrame is missing expected text fields: {missing_fields}\")","","        # Remove special characters","        for field in text_fields:","            self.dataframe[field] = self.dataframe[field].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', str(x)))","","        # Fill missing values","        self.dataframe[text_fields] = self.dataframe[text_fields].fillna('missing')","","        # Convert to lowercase","        self.dataframe[text_fields] = self.dataframe[text_fields].apply(lambda x: x.str.lower())","","        print(f\"Cleaned text fields: {text_fields}\")","","    def convert_response_delay(self):","        if 'response_delay' not in self.dataframe.columns:","            raise ValueError(\"DataFrame is missing the 'response_delay' column\")","","        self.dataframe['response_delay_hours'] = self.dataframe['response_delay'] / 3600","        print(f\"Created 'response_delay_hours' column\")"]}],[{"start":{"row":104,"column":0},"end":{"row":109,"column":20},"action":"remove","lines":["        # Calculate sentiment distribution per hourly bin","        sentiment_distribution = self.dataframe.groupby('hourly_bins').agg({","            'is_positive': 'sum',","            'is_neutral': 'sum',","            'is_negative': 'sum'","        }).fillna(0)"],"id":494},{"start":{"row":104,"column":0},"end":{"row":109,"column":12},"action":"insert","lines":["# Calculate sentiment distribution per hourly bin, explicitly setting observed parameter","sentiment_distribution = self.dataframe.groupby('hourly_bins', observed=True).agg({","    'is_positive': 'sum',","    'is_neutral': 'sum',","    'is_negative': 'sum'","}).fillna(0)"]}],[{"start":{"row":104,"column":0},"end":{"row":104,"column":4},"action":"insert","lines":["    "],"id":495},{"start":{"row":105,"column":0},"end":{"row":105,"column":4},"action":"insert","lines":["    "]},{"start":{"row":106,"column":0},"end":{"row":106,"column":4},"action":"insert","lines":["    "]},{"start":{"row":107,"column":0},"end":{"row":107,"column":4},"action":"insert","lines":["    "]},{"start":{"row":108,"column":0},"end":{"row":108,"column":4},"action":"insert","lines":["    "]},{"start":{"row":109,"column":0},"end":{"row":109,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":104,"column":0},"end":{"row":104,"column":4},"action":"insert","lines":["    "],"id":496},{"start":{"row":105,"column":0},"end":{"row":105,"column":4},"action":"insert","lines":["    "]},{"start":{"row":106,"column":0},"end":{"row":106,"column":4},"action":"insert","lines":["    "]},{"start":{"row":107,"column":0},"end":{"row":107,"column":4},"action":"insert","lines":["    "]},{"start":{"row":108,"column":0},"end":{"row":108,"column":4},"action":"insert","lines":["    "]},{"start":{"row":109,"column":0},"end":{"row":109,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":45,"column":55},"end":{"row":46,"column":0},"action":"insert","lines":["",""],"id":497},{"start":{"row":46,"column":0},"end":{"row":46,"column":8},"action":"insert","lines":["        "]},{"start":{"row":46,"column":8},"end":{"row":47,"column":0},"action":"insert","lines":["",""]},{"start":{"row":47,"column":0},"end":{"row":47,"column":8},"action":"insert","lines":["        "]}],[{"start":{"row":47,"column":4},"end":{"row":47,"column":8},"action":"remove","lines":["    "],"id":498}],[{"start":{"row":47,"column":4},"end":{"row":55,"column":24},"action":"insert","lines":["def restore_capitalization_and_periods(text):","    # Tokenize the text into sentences","    sentences = sent_tokenize(text)","    # Capitalize the first letter of each sentence and ensure it ends with a period","    restored_sentences = [sentence[0].upper() + sentence[1:] for sentence in sentences]","    restored_sentences = [sentence if sentence.endswith('.') else sentence + '.' for sentence in restored_sentences]","    # Join the sentences back to form the full text","    restored_text = ' '.join(restored_sentences)","    return restored_text"],"id":499}],[{"start":{"row":47,"column":0},"end":{"row":55,"column":24},"action":"remove","lines":["    def restore_capitalization_and_periods(text):","    # Tokenize the text into sentences","    sentences = sent_tokenize(text)","    # Capitalize the first letter of each sentence and ensure it ends with a period","    restored_sentences = [sentence[0].upper() + sentence[1:] for sentence in sentences]","    restored_sentences = [sentence if sentence.endswith('.') else sentence + '.' for sentence in restored_sentences]","    # Join the sentences back to form the full text","    restored_text = ' '.join(restored_sentences)","    return restored_text"],"id":500},{"start":{"row":46,"column":8},"end":{"row":47,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":186,"column":0},"end":{"row":186,"column":4},"action":"remove","lines":["    "],"id":501}],[{"start":{"row":186,"column":0},"end":{"row":187,"column":0},"action":"insert","lines":["",""],"id":502},{"start":{"row":187,"column":0},"end":{"row":188,"column":0},"action":"insert","lines":["",""]}],[{"start":{"row":188,"column":0},"end":{"row":196,"column":24},"action":"insert","lines":["    def restore_capitalization_and_periods(text):","    # Tokenize the text into sentences","    sentences = sent_tokenize(text)","    # Capitalize the first letter of each sentence and ensure it ends with a period","    restored_sentences = [sentence[0].upper() + sentence[1:] for sentence in sentences]","    restored_sentences = [sentence if sentence.endswith('.') else sentence + '.' for sentence in restored_sentences]","    # Join the sentences back to form the full text","    restored_text = ' '.join(restored_sentences)","    return restored_text"],"id":503}],[{"start":{"row":188,"column":0},"end":{"row":188,"column":4},"action":"remove","lines":["    "],"id":504}],[{"start":{"row":197,"column":0},"end":{"row":198,"column":0},"action":"insert","lines":["",""],"id":505}],[{"start":{"row":55,"column":0},"end":{"row":66,"column":173},"action":"remove","lines":["    def categorize_sentiment(self):","        # Use NLTK's SentimentIntensityAnalyzer to categorize sentiment","        def categorize(row):","            score = self.analyzer.polarity_scores(row)['compound']","            if score > 0.05:","                return 1, 0, 0  # Positive","            elif score < -0.05:","                return 0, 0, 1  # Negative","            else:","                return 0, 1, 0  # Neutral","        ","        self.dataframe[['is_positive', 'is_neutral', 'is_negative']] = self.dataframe.apply(lambda row: categorize(row['question_1_response']), axis=1, result_type='expand')"],"id":506},{"start":{"row":55,"column":0},"end":{"row":78,"column":113},"action":"insert","lines":["def categorize_sentiment(self):","    # Initialize counters for each sentiment","    positive_count = 0","    neutral_count = 0","    negative_count = 0","","    # Use NLTK's SentimentIntensityAnalyzer to categorize sentiment","    def categorize(row):","        nonlocal positive_count, neutral_count, negative_count","        score = self.analyzer.polarity_scores(row)['compound']","        if score > 0.05:","            positive_count += 1","            return 1, 0, 0  # Positive","        elif score < -0.05:","            negative_count += 1","            return 0, 0, 1  # Negative","        else:","            neutral_count += 1","            return 0, 1, 0  # Neutral","","    self.dataframe[['is_positive', 'is_neutral', 'is_negative']] = self.dataframe.apply(lambda row: categorize(row['question_1_response']), axis=1, result_type='expand')","","    # Print the counts","    print(f\"Sentiment counts - Positive: {positive_count}, Neutral: {neutral_count}, Negative: {negative_count}\")"]}],[{"start":{"row":55,"column":0},"end":{"row":55,"column":4},"action":"insert","lines":["    "],"id":507},{"start":{"row":56,"column":0},"end":{"row":56,"column":4},"action":"insert","lines":["    "]},{"start":{"row":57,"column":0},"end":{"row":57,"column":4},"action":"insert","lines":["    "]},{"start":{"row":58,"column":0},"end":{"row":58,"column":4},"action":"insert","lines":["    "]},{"start":{"row":59,"column":0},"end":{"row":59,"column":4},"action":"insert","lines":["    "]},{"start":{"row":60,"column":0},"end":{"row":60,"column":4},"action":"insert","lines":["    "]},{"start":{"row":61,"column":0},"end":{"row":61,"column":4},"action":"insert","lines":["    "]},{"start":{"row":62,"column":0},"end":{"row":62,"column":4},"action":"insert","lines":["    "]},{"start":{"row":63,"column":0},"end":{"row":63,"column":4},"action":"insert","lines":["    "]},{"start":{"row":64,"column":0},"end":{"row":64,"column":4},"action":"insert","lines":["    "]},{"start":{"row":65,"column":0},"end":{"row":65,"column":4},"action":"insert","lines":["    "]},{"start":{"row":66,"column":0},"end":{"row":66,"column":4},"action":"insert","lines":["    "]},{"start":{"row":67,"column":0},"end":{"row":67,"column":4},"action":"insert","lines":["    "]},{"start":{"row":68,"column":0},"end":{"row":68,"column":4},"action":"insert","lines":["    "]},{"start":{"row":69,"column":0},"end":{"row":69,"column":4},"action":"insert","lines":["    "]},{"start":{"row":70,"column":0},"end":{"row":70,"column":4},"action":"insert","lines":["    "]},{"start":{"row":71,"column":0},"end":{"row":71,"column":4},"action":"insert","lines":["    "]},{"start":{"row":72,"column":0},"end":{"row":72,"column":4},"action":"insert","lines":["    "]},{"start":{"row":73,"column":0},"end":{"row":73,"column":4},"action":"insert","lines":["    "]},{"start":{"row":74,"column":0},"end":{"row":74,"column":4},"action":"insert","lines":["    "]},{"start":{"row":75,"column":0},"end":{"row":75,"column":4},"action":"insert","lines":["    "]},{"start":{"row":76,"column":0},"end":{"row":76,"column":4},"action":"insert","lines":["    "]},{"start":{"row":77,"column":0},"end":{"row":77,"column":4},"action":"insert","lines":["    "]},{"start":{"row":78,"column":0},"end":{"row":78,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":123,"column":0},"end":{"row":123,"column":4},"action":"insert","lines":["    "],"id":508}],[{"start":{"row":123,"column":4},"end":{"row":123,"column":8},"action":"insert","lines":["    "],"id":509}],[{"start":{"row":123,"column":8},"end":{"row":128,"column":40},"action":"insert","lines":["# Calculate the total responses per bin for the running total","self.dataframe['total_responses'] = self.dataframe['is_positive'] + self.dataframe['is_neutral'] + self.dataframe['is_negative']","total_responses_per_bin = self.dataframe.groupby('hourly_bins')['total_responses'].sum().cumsum()","","# Plotting","fig, ax1 = plt.subplots(figsize=(14, 8))"],"id":510}],[{"start":{"row":127,"column":0},"end":{"row":128,"column":40},"action":"remove","lines":["# Plotting","fig, ax1 = plt.subplots(figsize=(14, 8))"],"id":511},{"start":{"row":126,"column":0},"end":{"row":127,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":124,"column":0},"end":{"row":124,"column":4},"action":"insert","lines":["    "],"id":512},{"start":{"row":125,"column":0},"end":{"row":125,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":124,"column":0},"end":{"row":124,"column":4},"action":"insert","lines":["    "],"id":513},{"start":{"row":125,"column":0},"end":{"row":125,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":139,"column":0},"end":{"row":140,"column":0},"action":"insert","lines":["",""],"id":514},{"start":{"row":140,"column":0},"end":{"row":141,"column":0},"action":"insert","lines":["",""]}],[{"start":{"row":141,"column":0},"end":{"row":141,"column":4},"action":"insert","lines":["    "],"id":515}],[{"start":{"row":141,"column":4},"end":{"row":141,"column":8},"action":"insert","lines":["    "],"id":516}],[{"start":{"row":141,"column":8},"end":{"row":152,"column":10},"action":"insert","lines":["# Add a line chart for the running total on the same axis","ax2 = ax1.twinx()  # Create a second y-axis that shares the same x-axis","ax2.plot(total_responses_per_bin.index, total_responses_per_bin, color='r', marker='o', linestyle='-', linewidth=2, markersize=5)","ax2.set_ylabel('Running Total of Responses', color='r')","ax2.tick_params(axis='y', labelcolor='r')","","# Optionally, adjust the x-axis to show all bins clearly","ax1.set_xticklabels(total_responses_per_bin.index, rotation=45, ha=\"right\")","","plt.title('Sentiment Distribution and Running Total of Responses Over Response Delay')","plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels","plt.show()"],"id":517}],[{"start":{"row":151,"column":0},"end":{"row":152,"column":10},"action":"remove","lines":["plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels","plt.show()"],"id":518}],[{"start":{"row":142,"column":0},"end":{"row":142,"column":4},"action":"insert","lines":["    "],"id":519},{"start":{"row":143,"column":0},"end":{"row":143,"column":4},"action":"insert","lines":["    "]},{"start":{"row":144,"column":0},"end":{"row":144,"column":4},"action":"insert","lines":["    "]},{"start":{"row":145,"column":0},"end":{"row":145,"column":4},"action":"insert","lines":["    "]},{"start":{"row":146,"column":0},"end":{"row":146,"column":4},"action":"insert","lines":["    "]},{"start":{"row":147,"column":0},"end":{"row":147,"column":4},"action":"insert","lines":["    "]},{"start":{"row":148,"column":0},"end":{"row":148,"column":4},"action":"insert","lines":["    "]},{"start":{"row":149,"column":0},"end":{"row":149,"column":4},"action":"insert","lines":["    "]},{"start":{"row":150,"column":0},"end":{"row":150,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":142,"column":0},"end":{"row":142,"column":4},"action":"insert","lines":["    "],"id":520},{"start":{"row":143,"column":0},"end":{"row":143,"column":4},"action":"insert","lines":["    "]},{"start":{"row":144,"column":0},"end":{"row":144,"column":4},"action":"insert","lines":["    "]},{"start":{"row":145,"column":0},"end":{"row":145,"column":4},"action":"insert","lines":["    "]},{"start":{"row":146,"column":0},"end":{"row":146,"column":4},"action":"insert","lines":["    "]},{"start":{"row":147,"column":0},"end":{"row":147,"column":4},"action":"insert","lines":["    "]},{"start":{"row":148,"column":0},"end":{"row":148,"column":4},"action":"insert","lines":["    "]},{"start":{"row":149,"column":0},"end":{"row":149,"column":4},"action":"insert","lines":["    "]},{"start":{"row":150,"column":0},"end":{"row":150,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":123,"column":1},"end":{"row":157,"column":55},"action":"remove","lines":["       # Calculate the total responses per bin for the running total","        self.dataframe['total_responses'] = self.dataframe['is_positive'] + self.dataframe['is_neutral'] + self.dataframe['is_negative']","        total_responses_per_bin = self.dataframe.groupby('hourly_bins')['total_responses'].sum().cumsum()","","        # Plotting","        fig, ax1 = plt.subplots(figsize=(14, 8))","        ","        # Colors for the stacked bar chart","        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Blue for positive, Orange for neutral, Green for negative","        ","        # Stacked bar chart for sentiment distribution per hour","        sentiment_distribution.plot(kind='bar', stacked=True, color=colors, ax=ax1)","        ax1.set_ylabel('Number of Responses')","        ax1.set_xlabel('Response Delay (Hours)')","        ax1.set_title('Sentiment Distribution Over Response Delay')","        ax1.legend([\"Positive\", \"Neutral\", \"Negative\"], title=\"Sentiment\")","","","        # Add a line chart for the running total on the same axis","        ax2 = ax1.twinx()  # Create a second y-axis that shares the same x-axis","        ax2.plot(total_responses_per_bin.index, total_responses_per_bin, color='r', marker='o', linestyle='-', linewidth=2, markersize=5)","        ax2.set_ylabel('Running Total of Responses', color='r')","        ax2.tick_params(axis='y', labelcolor='r')","        ","        # Optionally, adjust the x-axis to show all bins clearly","        ax1.set_xticklabels(total_responses_per_bin.index, rotation=45, ha=\"right\")","        ","        plt.title('Sentiment Distribution and Running Total of Responses Over Response Delay')","","        plt.xticks(rotation=45, ha=\"right\")","        plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels","        ","        plt.savefig('sentiment_distribution_over_response_delay.png')  # Save the plot to a file","        plt.show()","        plt.close()  # Close the plot to free up memory"],"id":521}],[{"start":{"row":123,"column":1},"end":{"row":123,"column":4},"action":"insert","lines":["   "],"id":522}],[{"start":{"row":123,"column":4},"end":{"row":123,"column":8},"action":"insert","lines":["    "],"id":523}],[{"start":{"row":123,"column":8},"end":{"row":124,"column":0},"action":"insert","lines":["",""],"id":524},{"start":{"row":124,"column":0},"end":{"row":124,"column":8},"action":"insert","lines":["        "]}],[{"start":{"row":124,"column":8},"end":{"row":149,"column":19},"action":"insert","lines":["        # Calculate the total responses per bin for the running total","        self.dataframe['total_responses'] = self.dataframe['is_positive'] + self.dataframe['is_neutral'] + self.dataframe['is_negative']","        total_responses_per_bin = self.dataframe.groupby('hourly_bins')['total_responses'].sum().cumsum()","","        # Plotting setup","        fig, ax1 = plt.subplots(figsize=(14, 8))","        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Define colors","        ","        # Plot bar chart","        sentiment_distribution.plot(kind='bar', stacked=True, color=colors, ax=ax1)","        ax1.set_ylabel('Number of Responses')","        ax1.set_xlabel('Response Delay (Hours)')","        ","        # Add line chart for the running total","        ax2 = ax1.twinx()","        ax2.plot(total_responses_per_bin.index, total_responses_per_bin, 'r-o', linewidth=2, markersize=5)","        ax2.set_ylabel('Running Total of Responses', color='r')","        ax2.tick_params(axis='y', labelcolor='r')","        ","        # Final adjustments","        plt.xticks(rotation=45, ha=\"right\")  # Ensure x-ticks are properly rotated","        plt.title('Sentiment Distribution and Running Total of Responses Over Response Delay')","        plt.tight_layout()  # Adjust layout","        plt.savefig('sentiment_distribution_over_response_delay.png')","        plt.show()","        plt.close()"],"id":525}],[{"start":{"row":183,"column":4},"end":{"row":191,"column":28},"action":"insert","lines":["    def restore_capitalization_and_periods(self, text):","        # Tokenize the text into sentences","        sentences = sent_tokenize(text)","        # Capitalize the first letter of each sentence and ensure it ends with a period","        restored_sentences = [sentence[0].upper() + sentence[1:] if len(sentence) > 0 else \"\" for sentence in sentences]","        restored_sentences = [sentence if sentence.endswith('.') else sentence + '.' for sentence in restored_sentences]","        # Join the sentences back to form the full text","        restored_text = ' '.join(restored_sentences)","        return restored_text"],"id":526}],[{"start":{"row":191,"column":28},"end":{"row":192,"column":0},"action":"insert","lines":["",""],"id":527},{"start":{"row":192,"column":0},"end":{"row":192,"column":8},"action":"insert","lines":["        "]}],[{"start":{"row":183,"column":4},"end":{"row":183,"column":8},"action":"remove","lines":["    "],"id":528}],[{"start":{"row":203,"column":0},"end":{"row":213,"column":32},"action":"remove","lines":["    def identify_highlight_reviews(self):","        # Assume generate_tag_counts has been called and we have dictionaries for tag counts","        positive_tags, neutral_tags, negative_tags = self.generate_tag_counts()","","        highlight_reviews = {","            'positive': self.find_highlight_review(positive_tags, self.dataframe['is_positive'] == 1),","            'neutral': self.find_highlight_review(neutral_tags, self.dataframe['is_neutral'] == 1),","            'negative': self.find_highlight_review(negative_tags, self.dataframe['is_negative'] == 1)","        }","","        return highlight_reviews"],"id":529},{"start":{"row":203,"column":0},"end":{"row":213,"column":32},"action":"insert","lines":["    def identify_highlight_reviews(self):","        # Assume generate_tag_counts has been called and we have dictionaries for tag counts","        positive_tags, neutral_tags, negative_tags = self.generate_tag_counts()","","        highlight_reviews = {","            'positive': self.restore_capitalization_and_periods(self.find_highlight_review(positive_tags, self.dataframe['is_positive'] == 1)),","            'neutral': self.restore_capitalization_and_periods(self.find_highlight_review(neutral_tags, self.dataframe['is_neutral'] == 1)),","            'negative': self.restore_capitalization_and_periods(self.find_highlight_review(negative_tags, self.dataframe['is_negative'] == 1))","        }","","        return highlight_reviews"]}],[{"start":{"row":216,"column":0},"end":{"row":224,"column":24},"action":"remove","lines":["def restore_capitalization_and_periods(text):","    # Tokenize the text into sentences","    sentences = sent_tokenize(text)","    # Capitalize the first letter of each sentence and ensure it ends with a period","    restored_sentences = [sentence[0].upper() + sentence[1:] for sentence in sentences]","    restored_sentences = [sentence if sentence.endswith('.') else sentence + '.' for sentence in restored_sentences]","    # Join the sentences back to form the full text","    restored_text = ' '.join(restored_sentences)","    return restored_text"],"id":530},{"start":{"row":215,"column":0},"end":{"row":216,"column":0},"action":"remove","lines":["",""]},{"start":{"row":214,"column":0},"end":{"row":215,"column":0},"action":"remove","lines":["",""]},{"start":{"row":213,"column":32},"end":{"row":214,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":157,"column":0},"end":{"row":181,"column":57},"action":"remove","lines":["    def generate_tag_counts(self):","        # Assuming 'tags' column contains comma-separated tags","        self.dataframe['tag_list'] = self.dataframe['tags'].str.split(', ')","        ","        # Initialize dictionaries to count tags for each sentiment","        positive_tags = {}","        neutral_tags = {}","        negative_tags = {}","        ","        # Iterate through DataFrame to populate dictionaries","        for index, row in self.dataframe.iterrows():","            for tag in row['tag_list']:","                if row['is_positive'] == 1:","                    positive_tags[tag] = positive_tags.get(tag, 0) + 1","                elif row['is_neutral'] == 1:","                    neutral_tags[tag] = neutral_tags.get(tag, 0) + 1","                elif row['is_negative'] == 1:","                    negative_tags[tag] = negative_tags.get(tag, 0) + 1","        ","        # Sort dictionaries by count in descending order","        positive_tags = dict(sorted(positive_tags.items(), key=lambda item: item[1], reverse=True))","        neutral_tags = dict(sorted(neutral_tags.items(), key=lambda item: item[1], reverse=True))","        negative_tags = dict(sorted(negative_tags.items(), key=lambda item: item[1], reverse=True))","        ","        return positive_tags, neutral_tags, negative_tags"],"id":531},{"start":{"row":157,"column":0},"end":{"row":186,"column":57},"action":"insert","lines":["    def generate_tag_counts(self):","        # Assuming 'tags' column contains comma-separated tags","        self.dataframe['tag_list'] = self.dataframe['tags'].str.split(', ')","        ","        # Initialize dictionaries to count tags for each sentiment","        positive_tags = {}","        neutral_tags = {}","        negative_tags = {}","        ","        # Iterate through DataFrame to populate dictionaries","        for index, row in self.dataframe.iterrows():","            for tag in row['tag_list']:","                if row['is_positive'] == 1:","                    positive_tags[tag] = positive_tags.get(tag, 0) + 1","                elif row['is_neutral'] == 1:","                    neutral_tags[tag] = neutral_tags.get(tag, 0) + 1","                elif row['is_negative'] == 1:","                    negative_tags[tag] = negative_tags.get(tag, 0) + 1","        ","        # Sort dictionaries by count in descending order","        positive_tags = dict(sorted(positive_tags.items(), key=lambda item: item[1], reverse=True))","        neutral_tags = dict(sorted(neutral_tags.items(), key=lambda item: item[1], reverse=True))","        negative_tags = dict(sorted(negative_tags.items(), key=lambda item: item[1], reverse=True))","        ","        # Print the top 5 tags for each sentiment","        print(\"Top 5 Positive Tags:\", list(positive_tags.items())[:5])","        print(\"Top 5 Neutral Tags:\", list(neutral_tags.items())[:5])","        print(\"Top 5 Negative Tags:\", list(negative_tags.items())[:5])","        ","        return positive_tags, neutral_tags, negative_tags"]}],[{"start":{"row":126,"column":8},"end":{"row":126,"column":105},"action":"remove","lines":["total_responses_per_bin = self.dataframe.groupby('hourly_bins')['total_responses'].sum().cumsum()"],"id":532},{"start":{"row":126,"column":8},"end":{"row":127,"column":0},"action":"insert","lines":["total_responses_per_bin = self.dataframe.groupby('hourly_bins', observed=True)['total_responses'].sum().cumsum()",""]}],[{"start":{"row":126,"column":120},"end":{"row":127,"column":0},"action":"remove","lines":["",""],"id":533}],[{"start":{"row":12,"column":0},"end":{"row":12,"column":2},"action":"insert","lines":["# "],"id":534},{"start":{"row":13,"column":0},"end":{"row":13,"column":2},"action":"insert","lines":["# "]},{"start":{"row":14,"column":0},"end":{"row":14,"column":2},"action":"insert","lines":["# "]}],[{"start":{"row":127,"column":0},"end":{"row":127,"column":4},"action":"insert","lines":["    "],"id":535}],[{"start":{"row":127,"column":4},"end":{"row":127,"column":8},"action":"insert","lines":["    "],"id":536}],[{"start":{"row":127,"column":8},"end":{"row":128,"column":0},"action":"insert","lines":["",""],"id":537},{"start":{"row":128,"column":0},"end":{"row":128,"column":8},"action":"insert","lines":["        "]}],[{"start":{"row":128,"column":8},"end":{"row":131,"column":9},"action":"insert","lines":["        plot_data = {","            'sentiment_distribution': sentiment_distribution.reset_index().to_dict(orient='records'),","            'total_responses_per_bin': total_responses_per_bin.reset_index().to_dict(orient='records'),","        }"],"id":538}],[{"start":{"row":128,"column":12},"end":{"row":128,"column":16},"action":"remove","lines":["    "],"id":539},{"start":{"row":128,"column":8},"end":{"row":128,"column":12},"action":"remove","lines":["    "]}],[{"start":{"row":155,"column":0},"end":{"row":155,"column":4},"action":"insert","lines":["    "],"id":540}],[{"start":{"row":155,"column":4},"end":{"row":155,"column":8},"action":"insert","lines":["    "],"id":541}],[{"start":{"row":155,"column":8},"end":{"row":155,"column":24},"action":"insert","lines":["return plot_data"],"id":542}],[{"start":{"row":155,"column":24},"end":{"row":156,"column":0},"action":"insert","lines":["",""],"id":543},{"start":{"row":156,"column":0},"end":{"row":156,"column":8},"action":"insert","lines":["        "]},{"start":{"row":156,"column":8},"end":{"row":157,"column":0},"action":"insert","lines":["",""]},{"start":{"row":157,"column":0},"end":{"row":157,"column":8},"action":"insert","lines":["        "]}],[{"start":{"row":157,"column":4},"end":{"row":157,"column":8},"action":"remove","lines":["    "],"id":544},{"start":{"row":157,"column":0},"end":{"row":157,"column":4},"action":"remove","lines":["    "]},{"start":{"row":156,"column":8},"end":{"row":157,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":226,"column":0},"end":{"row":265,"column":51},"action":"remove","lines":["def main():","    try:","        # Step 1: Load the dataset","        dataframe = pd.read_csv('twilio.csv')","    except FileNotFoundError:","        print(\"Error: The dataset file 'twilio.csv' was not found.\")","        return","    except Exception as e:","        print(f\"An unexpected error occurred while loading the dataset: {e}\")","        return","","    try:","        # Initialize the classes with the DataFrame","        preprocessor = DataPreprocessor(dataframe)","        sentiment_analyzer = SentimentAnalyzer(dataframe)","        aspect_extractor = AspectExtractor(dataframe)","        visualizer = DataVisualizer(dataframe)","","        # Step 2: Preprocess the data","        preprocessor.clean_data()","        preprocessor.convert_response_delay()","","        # Step 3: Analyze sentiments","        sentiment_analyzer.categorize_sentiment()","","        # Step 4: Extract aspects","        aspect_extractor.extract_aspects()","","        # Step 5: Visualize data","        visualizer.visualize_sentiments_over_delay()","","        # Optional Steps:","        # Generate tag counts and identify highlight reviews","        tag_counts = visualizer.generate_tag_counts()","        highlight_reviews = visualizer.identify_highlight_reviews()","","        # Export results to JSON (example for highlight reviews)","        visualizer.export_to_json(highlight_reviews, 'highlight_reviews.json')","    except Exception as e:","        print(f\"An unexpected error occurred: {e}\")"],"id":545},{"start":{"row":226,"column":0},"end":{"row":284,"column":10},"action":"insert","lines":["def main():","    # Download necessary NLTK resources","    nltk.download('averaged_perceptron_tagger', quiet=True)","    nltk.download('punkt', quiet=True)","    nltk.download('stopwords', quiet=True)","    nltk.download('vader_lexicon', quiet=True)  # Ensure the sentiment analysis lexicon is also downloaded","","    try:","        dataframe = pd.read_csv('twilio.csv')","    except FileNotFoundError:","        print(\"Error: The dataset file 'twilio.csv' was not found.\")","        return","    except Exception as e:","        print(f\"An unexpected error occurred while loading the dataset: {e}\")","        return","","    try:","        preprocessor = DataPreprocessor(dataframe)","        sentiment_analyzer = SentimentAnalyzer(dataframe)","        aspect_extractor = AspectExtractor(dataframe)","        visualizer = DataVisualizer(dataframe)","","        output_data = {}","","        preprocessor.clean_data()","        preprocessor.convert_response_delay()","","        sentiment_analyzer.categorize_sentiment()","        output_data['sentiment_counts'] = {","            'positive': dataframe['is_positive'].sum(),","            'neutral': dataframe['is_neutral'].sum(),","            'negative': dataframe['is_negative'].sum(),","        }","","        aspect_extractor.extract_aspects()","","        # Capture plot data","        plot_data = visualizer.visualize_sentiments_over_delay()","        output_data['plot_data'] = plot_data","","        # Generate tag counts and identify highlight reviews","        top_tags = visualizer.generate_tag_counts()","        output_data['top_5_tags'] = {","            'positive': list(top_tags[0].items())[:5],","            'neutral': list(top_tags[1].items())[:5],","            'negative': list(top_tags[2].items())[:5],","        }","","        highlight_reviews = visualizer.identify_highlight_reviews()","        output_data['highlight_reviews'] = highlight_reviews","","        visualizer.export_to_json(output_data, 'analysis_results.json')","        print(\"Analysis results exported to 'analysis_results.json'.\")","        ","    except Exception as e:","        print(f\"An unexpected error occurred: {e}\")","","if __name__ == '__main__':","    main()"]}],[{"start":{"row":12,"column":0},"end":{"row":14,"column":40},"action":"remove","lines":["# nltk.download('averaged_perceptron_tagger', quiet=True)","# nltk.download('punkt', quiet=True)","# nltk.download('stopwords', quiet=True)"],"id":546},{"start":{"row":11,"column":28},"end":{"row":12,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":283,"column":0},"end":{"row":284,"column":10},"action":"remove","lines":["if __name__ == '__main__':","    main()"],"id":547},{"start":{"row":282,"column":0},"end":{"row":283,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":222,"column":0},"end":{"row":223,"column":0},"action":"insert","lines":["",""],"id":548},{"start":{"row":223,"column":0},"end":{"row":224,"column":0},"action":"insert","lines":["",""]}],[{"start":{"row":222,"column":0},"end":{"row":226,"column":19},"action":"insert","lines":["# Before calling json.dump, convert numpy.int64 to int","def convert_numpy_int64(o):","    if isinstance(o, np.int64):","        return int(o)","    raise TypeError"],"id":549}],[{"start":{"row":154,"column":45},"end":{"row":155,"column":0},"action":"insert","lines":["",""],"id":550},{"start":{"row":155,"column":0},"end":{"row":155,"column":8},"action":"insert","lines":["        "]}],[{"start":{"row":155,"column":8},"end":{"row":155,"column":85},"action":"insert","lines":["json.dump(data, f, ensure_ascii=False, indent=4, default=convert_numpy_int64)"],"id":551}],[{"start":{"row":155,"column":8},"end":{"row":155,"column":85},"action":"remove","lines":["json.dump(data, f, ensure_ascii=False, indent=4, default=convert_numpy_int64)"],"id":552},{"start":{"row":155,"column":4},"end":{"row":155,"column":8},"action":"remove","lines":["    "]},{"start":{"row":155,"column":0},"end":{"row":155,"column":4},"action":"remove","lines":["    "]},{"start":{"row":154,"column":45},"end":{"row":155,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":156,"column":60},"end":{"row":157,"column":0},"action":"insert","lines":["",""],"id":553},{"start":{"row":157,"column":0},"end":{"row":157,"column":12},"action":"insert","lines":["            "]}],[{"start":{"row":157,"column":8},"end":{"row":157,"column":12},"action":"remove","lines":["    "],"id":554},{"start":{"row":157,"column":4},"end":{"row":157,"column":8},"action":"remove","lines":["    "]},{"start":{"row":157,"column":0},"end":{"row":157,"column":4},"action":"remove","lines":["    "]},{"start":{"row":156,"column":60},"end":{"row":157,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":156,"column":12},"end":{"row":156,"column":60},"action":"remove","lines":["json.dump(data, f, ensure_ascii=False, indent=4)"],"id":555},{"start":{"row":156,"column":12},"end":{"row":156,"column":89},"action":"insert","lines":["json.dump(data, f, ensure_ascii=False, indent=4, default=convert_numpy_int64)"]}],[{"start":{"row":281,"column":8},"end":{"row":281,"column":10},"action":"insert","lines":["# "],"id":556}],[{"start":{"row":184,"column":8},"end":{"row":184,"column":10},"action":"insert","lines":["# "],"id":557},{"start":{"row":185,"column":8},"end":{"row":185,"column":10},"action":"insert","lines":["# "]},{"start":{"row":186,"column":8},"end":{"row":186,"column":10},"action":"insert","lines":["# "]}],[{"start":{"row":75,"column":8},"end":{"row":75,"column":10},"action":"insert","lines":["# "],"id":558}],[{"start":{"row":42,"column":8},"end":{"row":42,"column":10},"action":"insert","lines":["# "],"id":559}],[{"start":{"row":35,"column":8},"end":{"row":35,"column":10},"action":"insert","lines":["# "],"id":560}]]},"ace":{"folds":[],"scrolltop":1149,"scrollleft":0,"selection":{"start":{"row":11,"column":28},"end":{"row":11,"column":28},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":{"row":70,"state":"start","mode":"ace/mode/python"}},"timestamp":1709390888506,"hash":"9118567ecbcbbf7973ae1d8ef588963e02d28b5d"}